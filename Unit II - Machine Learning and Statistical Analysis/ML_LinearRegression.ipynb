{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%logstop\n",
    "%logstart -ortq ~/.logs/ML_LinearRegression.py append\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression and Machine Learning\n",
    "\n",
    "<!-- requirement: data/gas_consumption.csv -->\n",
    "\n",
    "The power of quantitative sciences comes from the insight we can derive from mathematical relationships between different measurements. We can use these insights to make predictions about what will happen in the future. The simplest possible relationship between two variables is a linear relationship\n",
    "\n",
    "$$y_i \\approx \\beta_0 + \\beta_1x_i$$\n",
    "\n",
    "If we can measure some $(x_i, y_i)$ pairs, we could calculate our _model parameters_ $\\beta_0$ and $\\beta_1$. Then we could predict $y$ in the future based on $x$, or even try to influence $y$ in the future by controlling $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas = pd.read_csv('./data/gas_consumption.csv', names=['tax', 'income', 'highway', 'drivers', 'gas'])\n",
    "gas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas.plot(x='drivers', y='gas', kind='scatter')\n",
    "plt.xlabel('% of population driving')\n",
    "plt.ylabel('Gas consumption (millions of gallons)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could try to draw a line describing the trend in the data, but which is the best one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas.plot(x='drivers', y='gas', kind='scatter')\n",
    "plt.xlabel('% of population driving')\n",
    "plt.ylabel('Gas consumption (millions gallons)')\n",
    "\n",
    "plt.plot([.4, .8], [300, 1000], 'r-')\n",
    "plt.plot([.4, .8], [200, 1100], 'g-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare the different trend lines we need to define a **metric** for how well they describe the actual data. The metric should reflect what we value about our trend line. We want our trend line to reliably predict a y-value given an x-value, so it would be reasonable to construct our metric based on the **error** between the trend line and the y-values.\n",
    "\n",
    "$$ e_i = y_i - (\\beta_0 + \\beta_1x_i) $$\n",
    "\n",
    "We want to make the total error as small as possible. Since sometimes the errors will be positive and some will be negative, if we add them together they might cancel out. We don't care if the error is positive or negative, we want the _absolute value_ to be small. Instead of minimizing the total error, we'll minimize the total squared error. Often we divide it by the number of data points, $n$, which is called the **mean squared error** (MSE).\n",
    "\n",
    "$$ MSE = \\frac{1}{n}\\sum_i e_i^2 $$\n",
    "\n",
    "Since $e_i$ depends on our model parameters $\\beta_0$ and $\\beta_1$, we can tweak our model (the trend line) until the MSE is minimized. In the language of machine learning, the MSE would be called the **cost function** or **loss function**. For different machine learning tasks, we will define different cost functions (or **objective functions/utility functions**, which we seek to maximize instead of minimize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression(fit_intercept=True)\n",
    "linreg.fit(gas[['drivers']], gas['gas'])\n",
    "\n",
    "gas.plot(x='drivers', y='gas', kind='scatter')\n",
    "plt.xlabel('% of population driving')\n",
    "plt.ylabel('Gas consumption (millions gallons)')\n",
    "\n",
    "x = np.linspace(.4, .8).reshape(-1, 1)\n",
    "print(x.shape)\n",
    "plt.plot(x, linreg.predict(x), 'k-')\n",
    "plt.plot([.4, .8], [300, 1000], 'r-')\n",
    "plt.plot([.4, .8], [200, 1100], 'g-');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(linreg.intercept_, linreg.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "How did we find the model parameters that minimize the cost function? Let's plot the cost function with respect to $\\beta_1$ to get an idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta0 = linreg.intercept_\n",
    "beta1 = np.linspace(1300, 1500)\n",
    "\n",
    "MSE = [((gas['gas'] - (beta0 + m * gas['drivers']))**2).sum() for m in beta1]\n",
    "\n",
    "plt.plot(beta1, MSE);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we started with some initial guess $\\beta_1 = 1300$, we could simply follow the slope of the MSE downhill with respect to $\\beta_1$. We could calculate the MSE around 1300 to work out which way is downhill, and then update $\\beta_1$ in that direction. With each step we move closer and closer to the bottom of the valley at 1409.\n",
    "\n",
    "This method of always going downhill from where we are is called **gradient descent**. In general the loss function could be very complicated and we won't be able to solve where the minimum is directly. Gradient descent gives us an algorithm for finding our way to the minimum when we don't know where it is in advance.\n",
    "\n",
    "For example, the `HuberRegressor` also optimizes a linear model, but [uses a more complicated loss function](http://scikit-learn.org/stable/modules/linear_model.html#huber-regression). The Huber loss is less influenced by outliers than the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import HuberRegressor\n",
    "huber = HuberRegressor(fit_intercept=True, alpha=0)\n",
    "huber.fit(gas[['drivers']], gas['gas'])\n",
    "gas.plot(x='drivers', y='gas', kind='scatter')\n",
    "plt.xlabel('% of population driving')\n",
    "plt.ylabel('Gas consumption (millions gallons)')\n",
    "\n",
    "x = np.linspace(.4, .8).reshape(-1, 1)\n",
    "plt.plot(x, linreg.predict(x), 'k-')\n",
    "plt.plot(x, huber.predict(x), 'm-')\n",
    "plt.legend(['Simple linear regression (MSE)', 'Huber regression']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate regression\n",
    "\n",
    "Looking again at our DataFrame, we see we have other variables we could use to predict gas consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "\n",
    "feature_desc = {'tax': 'Gas tax', 'drivers': '% of population driving', 'income': 'Average income (USD)', 'highway': 'Miles of paved highway'}\n",
    "def plot_feature(column):\n",
    "    plt.plot(gas[column], gas['gas'], '.')\n",
    "    plt.xlabel(feature_desc[column])\n",
    "    plt.ylabel('Gas consumption (millions gallons)')\n",
    "\n",
    "dropdown_menu = {v: k for k, v in feature_desc.items()}\n",
    "\n",
    "widgets.interact(plot_feature, column=dropdown_menu);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use all of these predictors (called **features**), we will need to fit a slightly more complicated function\n",
    "\n",
    "$$ y_i \\approx \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} + \\beta_3x_{3i} + \\beta_4x_{4i} $$\n",
    "\n",
    "or more generally\n",
    "\n",
    "$$ y_i \\approx  \\sum_j\\beta_jX_{ij} $$\n",
    "\n",
    "where $i$ labels different **observations** and $j$ labels different **features**. When we have one feature, we solve for a line; when we have two features, we solve for a plane; and so on, even if we can't imagine higher dimensional spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt3d = plt.figure().gca(projection='3d')\n",
    "plt3d.scatter(gas['tax'], gas['drivers'], gas['gas']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.fit(gas[['tax', 'drivers']], gas['gas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt3d = plt.figure().gca(projection='3d')\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(5, 11), np.linspace(.4, .8))\n",
    "z = linreg.intercept_ + linreg.coef_[0] * xx + linreg.coef_[1] * yy\n",
    "plt3d.plot_surface(xx, yy, z, alpha=0.2)\n",
    "plt3d.scatter(gas['tax'], gas['drivers'], gas['gas']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "def plot_cross(tax=5):\n",
    "    x = np.linspace(.4, .8)\n",
    "    plt.plot(x, linreg.intercept_ + linreg.coef_[0]*tax + linreg.coef_[1]*x)\n",
    "    alpha = 1 - abs(gas['tax'] - tax) / abs(gas['tax'] - tax).max()\n",
    "    colors = np.zeros((len(gas), 4))\n",
    "    colors[:, 3] = alpha\n",
    "    plt.scatter(gas['drivers'], gas['gas'], color=colors)\n",
    "\n",
    "interact(plot_cross, tax=(5,11,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
